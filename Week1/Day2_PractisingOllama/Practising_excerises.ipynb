{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3601072f",
   "metadata": {},
   "source": [
    "# Exercise Solution and Practising\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source Model by using locally Ollama rather using OpenAI \n",
    "\n",
    "If you don't want to use paid models. Then, You'll be able to use this technique for all things and projects.\n",
    "\n",
    "**Pros:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Cons:**\n",
    "1. Less powerful than paid models \n",
    "\n",
    "## Recap on installation of Ollama \n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install! \n",
    "\n",
    "Once complete, the ollama server should already by running locally, \n",
    "\n",
    "If you visit, \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "you should see the message \"Ollama is running\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb7162",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7119ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d9c1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"Create this Website object from the given url using the Beautiful soup\"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True )\n",
    "        self.text = '\\n'.join(self.text.splitlines()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2b268c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3\n",
      "Models\n",
      "GitHub\n",
      "Discord\n",
      "Turbo\n",
      "Sign in\n",
      "Download\n",
      "Models\n",
      "Download\n",
      "GitHub\n",
      "Discord\n",
      "Sign in\n",
      "qwen3\n",
      "8M\n",
      "Downloads\n",
      "Updated\n",
      "1 month ago\n",
      "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.\n",
      "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.\n",
      "Cancel\n",
      "tools\n",
      "thinking\n",
      "0.6b\n",
      "1.7b\n",
      "4b\n",
      "8b\n",
      "14b\n",
      "30b\n",
      "32b\n",
      "235b\n",
      "Models\n",
      "View all →\n",
      "Name\n",
      "56 models\n",
      "Size\n",
      "Context\n",
      "Input\n",
      "qwen3:latest\n",
      "5.2GB · 40K context window · Text · 3 months ago\n",
      "qwen3:latest\n",
      "5.2GB\n",
      "40K\n",
      "Text\n",
      "qwen3:0.6b\n",
      "523MB · 40K context window · Text · 3 months ago\n",
      "qwen3:0.6b\n",
      "523MB\n",
      "40K\n",
      "Text\n",
      "qwen3:1.7b\n",
      "1.4GB · 40K context window · Text · 3 months ago\n",
      "qwen3:1.7b\n",
      "1.4GB\n",
      "40K\n",
      "Text\n",
      "qwen3:4b\n",
      "2.5GB · 256K context window · Text · 1 month ago\n",
      "qwen3:4b\n",
      "2.5GB\n",
      "256K\n",
      "Text\n",
      "qwen3:8b\n",
      "latest\n",
      "5.2GB · 40K context window · Text · 3 months ago\n",
      "qwen3:8b\n",
      "latest\n",
      "5.2GB\n",
      "40K\n",
      "Text\n",
      "qwen3:14b\n",
      "9.3GB · 40K context window · Text · 3 months ago\n",
      "qwen3:14b\n",
      "9.3GB\n",
      "40K\n",
      "Text\n",
      "qwen3:30b\n",
      "19GB · 256K context window · Text · 1 month ago\n",
      "qwen3:30b\n",
      "19GB\n",
      "256K\n",
      "Text\n",
      "qwen3:32b\n",
      "20GB · 40K context window · Text · 3 months ago\n",
      "qwen3:32b\n",
      "20GB\n",
      "40K\n",
      "Text\n",
      "qwen3:235b\n",
      "142GB · 256K context window · Text · 1 month ago\n",
      "qwen3:235b\n",
      "142GB\n",
      "256K\n",
      "Text\n",
      "Readme\n",
      "Qwen 3\n",
      "is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\n",
      "New 30B model\n",
      "ollama run qwen3:30b\n",
      "New 235B model\n",
      "ollama run qwen3:235b\n",
      "Overview\n",
      "The Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model,\n",
      "Qwen3-235B-A22B\n",
      ", achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model,\n",
      "Qwen3-30B-A3B\n",
      ", outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\n",
      "Significantly enhancement in its reasoning capabilities\n",
      ", surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n",
      "Superior human preference alignment\n",
      ", excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n",
      "Expertise in agent capabilities\n",
      ", enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n",
      "Support of 100+ languages and dialects\n",
      "with strong capabilities for\n",
      "multilingual instruction following and translation\n",
      ".\n",
      "Reference\n",
      "Blog\n",
      "Write\n",
      "Preview\n",
      "![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522)\n",
      "\n",
      "**Qwen 3** is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\n",
      "\n",
      "### New 30B model\n",
      "\n",
      "```\n",
      "ollama run qwen3:30b\n",
      "```\n",
      "\n",
      "![Qwen3-30B-A3B-Instruct-2507.jpg](/assets/library/qwen3/bc0ddfea-95b5-49fc-a36e-c817f98a5de0)\n",
      "\n",
      "\n",
      "### New 235B model\n",
      "\n",
      "```\n",
      "ollama run qwen3:235b\n",
      "```\n",
      "\n",
      "![0d7zztq4GB7G2ZYowO-dQ.jpg](/assets/library/qwen3/8426a459-dd88-49cd-ae89-ece442e58ec5)\n",
      "\n",
      "### Overview\n",
      "\n",
      "The Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model, **Qwen3-235B-A22B**, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, **Qwen3-30B-A3B**, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\n",
      "\n",
      "- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n",
      "\n",
      "- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n",
      "\n",
      "- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n",
      "\n",
      "- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following and translation**.\n",
      "\n",
      "### Reference\n",
      "- [Blog](https://qwenlm.github.io/blog/qwen3/)\n",
      "Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)\n",
      "© 2025 Ollama\n",
      "Download\n",
      "Blog\n",
      "Docs\n",
      "GitHub\n",
      "Discord\n",
      "X (Twitter)\n",
      "Contact Us\n",
      "Blog\n",
      "Download\n",
      "Docs\n",
      "GitHub\n",
      "Discord\n",
      "X (Twitter)\n",
      "Meetups\n",
      "© 2025 Ollama Inc.\n"
     ]
    }
   ],
   "source": [
    "er = website(\"https://ollama.com/library/qwen3\")\n",
    "print(er.title)\n",
    "print(er.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4597286",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Hello Qwen3 you will get text from the website content, \" \\\n",
    "\"you will summarize the whole content and ignore if you see any other unrelated content and response in Markdown\"\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"you are looking at website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b171aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are looking at website titled qwen3\n",
      "The contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\n",
      "\n",
      "Models\n",
      "GitHub\n",
      "Discord\n",
      "Turbo\n",
      "Sign in\n",
      "Download\n",
      "Models\n",
      "Download\n",
      "GitHub\n",
      "Discord\n",
      "Sign in\n",
      "qwen3\n",
      "8M\n",
      "Downloads\n",
      "Updated\n",
      "1 month ago\n",
      "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.\n",
      "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.\n",
      "Cancel\n",
      "tools\n",
      "thinking\n",
      "0.6b\n",
      "1.7b\n",
      "4b\n",
      "8b\n",
      "14b\n",
      "30b\n",
      "32b\n",
      "235b\n",
      "Models\n",
      "View all →\n",
      "Name\n",
      "56 models\n",
      "Size\n",
      "Context\n",
      "Input\n",
      "qwen3:latest\n",
      "5.2GB · 40K context window · Text · 3 months ago\n",
      "qwen3:latest\n",
      "5.2GB\n",
      "40K\n",
      "Text\n",
      "qwen3:0.6b\n",
      "523MB · 40K context window · Text · 3 months ago\n",
      "qwen3:0.6b\n",
      "523MB\n",
      "40K\n",
      "Text\n",
      "qwen3:1.7b\n",
      "1.4GB · 40K context window · Text · 3 months ago\n",
      "qwen3:1.7b\n",
      "1.4GB\n",
      "40K\n",
      "Text\n",
      "qwen3:4b\n",
      "2.5GB · 256K context window · Text · 1 month ago\n",
      "qwen3:4b\n",
      "2.5GB\n",
      "256K\n",
      "Text\n",
      "qwen3:8b\n",
      "latest\n",
      "5.2GB · 40K context window · Text · 3 months ago\n",
      "qwen3:8b\n",
      "latest\n",
      "5.2GB\n",
      "40K\n",
      "Text\n",
      "qwen3:14b\n",
      "9.3GB · 40K context window · Text · 3 months ago\n",
      "qwen3:14b\n",
      "9.3GB\n",
      "40K\n",
      "Text\n",
      "qwen3:30b\n",
      "19GB · 256K context window · Text · 1 month ago\n",
      "qwen3:30b\n",
      "19GB\n",
      "256K\n",
      "Text\n",
      "qwen3:32b\n",
      "20GB · 40K context window · Text · 3 months ago\n",
      "qwen3:32b\n",
      "20GB\n",
      "40K\n",
      "Text\n",
      "qwen3:235b\n",
      "142GB · 256K context window · Text · 1 month ago\n",
      "qwen3:235b\n",
      "142GB\n",
      "256K\n",
      "Text\n",
      "Readme\n",
      "Qwen 3\n",
      "is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\n",
      "New 30B model\n",
      "ollama run qwen3:30b\n",
      "New 235B model\n",
      "ollama run qwen3:235b\n",
      "Overview\n",
      "The Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model,\n",
      "Qwen3-235B-A22B\n",
      ", achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model,\n",
      "Qwen3-30B-A3B\n",
      ", outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\n",
      "Significantly enhancement in its reasoning capabilities\n",
      ", surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n",
      "Superior human preference alignment\n",
      ", excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n",
      "Expertise in agent capabilities\n",
      ", enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n",
      "Support of 100+ languages and dialects\n",
      "with strong capabilities for\n",
      "multilingual instruction following and translation\n",
      ".\n",
      "Reference\n",
      "Blog\n",
      "Write\n",
      "Preview\n",
      "![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522)\n",
      "\n",
      "**Qwen 3** is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models:\n",
      "\n",
      "### New 30B model\n",
      "\n",
      "```\n",
      "ollama run qwen3:30b\n",
      "```\n",
      "\n",
      "![Qwen3-30B-A3B-Instruct-2507.jpg](/assets/library/qwen3/bc0ddfea-95b5-49fc-a36e-c817f98a5de0)\n",
      "\n",
      "\n",
      "### New 235B model\n",
      "\n",
      "```\n",
      "ollama run qwen3:235b\n",
      "```\n",
      "\n",
      "![0d7zztq4GB7G2ZYowO-dQ.jpg](/assets/library/qwen3/8426a459-dd88-49cd-ae89-ece442e58ec5)\n",
      "\n",
      "### Overview\n",
      "\n",
      "The Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model, **Qwen3-235B-A22B**, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, **Qwen3-30B-A3B**, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.\n",
      "\n",
      "- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n",
      "\n",
      "- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n",
      "\n",
      "- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n",
      "\n",
      "- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following and translation**.\n",
      "\n",
      "### Reference\n",
      "- [Blog](https://qwenlm.github.io/blog/qwen3/)\n",
      "Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)\n",
      "© 2025 Ollama\n",
      "Download\n",
      "Blog\n",
      "Docs\n",
      "GitHub\n",
      "Discord\n",
      "X (Twitter)\n",
      "Contact Us\n",
      "Blog\n",
      "Download\n",
      "Docs\n",
      "GitHub\n",
      "Discord\n",
      "X (Twitter)\n",
      "Meetups\n",
      "© 2025 Ollama Inc.\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_for(er))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "792e4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "    {\"role\": \"system\", \"messages\": system_prompt},\n",
    "    {\"role\": \"user\", \"messages\": user_prompt_for(website)}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72ae3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url, website):\n",
    "    website = website(url)\n",
    "    response = ollama.chat(\n",
    "        model =\"qwen3\", \n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39d12caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user sent an empty message. I need to respond appropriately. Since there's no content, I should ask them what they need help with. Keep it friendly and open-ended. Maybe something like, \"Hello! How can I assist you today?\" That should encourage them to provide more details so I can help effectively.\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? 😊\n"
     ]
    }
   ],
   "source": [
    "output = summarize(\"https://ollama.com/library/qwen3\", website)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
